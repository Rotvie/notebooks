{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3423f04e",
   "metadata": {},
   "source": [
    "# Train a Custom NER Model with SpaCy\n",
    "\n",
    "This notebook demonstrates how to train a custom Named Entity Recognition (NER) model using the SpaCy library.\n",
    "\n",
    "## What is Named Entity Recognition?\n",
    "\n",
    "Named Entity Recognition (NER) is a natural language processing task that identifies and classifies entities in text into predefined categories such as:\n",
    "- **People** (PER)\n",
    "- **Organizations** (ORG)\n",
    "- **Locations** (LOC)\n",
    "- **Dates** (DATE)\n",
    "- And other custom categories\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial will cover:\n",
    "1. Installing required dependencies\n",
    "2. Loading and preparing annotated training data\n",
    "3. Converting data to SpaCy format\n",
    "4. Training a custom NER model\n",
    "5. Running inference on new text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j0iqspxlbui",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "First, we need to install SpaCy and the transformer components for better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65e2138",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U spacy\n",
    "!pip install spacy-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t2x80xsdh7o",
   "metadata": {},
   "source": [
    "## Step 2: Setup Environment and Import Libraries\n",
    "\n",
    "Mount Google Drive to access your annotated dataset and import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e72fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive to access project files\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Change to your project directory\n",
    "%cd \"/content/drive/MyDrive/\"\n",
    "\n",
    "# Create the directory tree \n",
    "!mkdir -p Custom_NER/annotations_dataset\n",
    "!mkdir -p Custom_NER/config\n",
    "!mkdir -p Custom_NER/trained_models\n",
    "\n",
    "# Import required libraries\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Check SpaCy version\n",
    "print(f\"SpaCy version: {spacy.__version__}\")\n",
    "\n",
    "# Check GPU availability (important for faster training)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qxbw953eum",
   "metadata": {},
   "source": [
    "## Step 3: Load Annotated Data\n",
    "\n",
    "Load your annotated dataset from a JSON file. The annotations should be in the format: `[[text, {\"entities\": [[start, end, label], ...]}], ...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682cc54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the annotated data from JSON file\n",
    "# Update this path to match your dataset location\n",
    "cv_data = json.load(open('/content/drive/MyDrive/Custom_NER/annotations_dataset/annotations.json', 'r', encoding='utf-8-sig'))\n",
    "\n",
    "# Display dataset statistics\n",
    "print(f\"Total number of annotated examples: {len(cv_data)}\")\n",
    "\n",
    "# Display the first example to verify the format\n",
    "print(\"\\nFirst example:\")\n",
    "print(cv_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0lcgdcx3eqph",
   "metadata": {},
   "source": [
    "## Step 4: Initialize SpaCy Configuration\n",
    "\n",
    "Create a configuration file for training. SpaCy requires a config file that defines the model architecture, training parameters, and hyperparameters. You can find the base configuration in the [SpaCy documentation](https://spacy.io/usage/training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9ce41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SpaCy config from base configuration\n",
    "# This fills in all default values for training\n",
    "!python -m spacy init fill-config /content/drive/MyDrive/Custom_NER/config/base_config.cfg /content/drive/MyDrive/Custom_NER/config/config.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v00y60pbw7p",
   "metadata": {},
   "source": [
    "## Step 5: Define Data Conversion Function\n",
    "\n",
    "This function converts your annotated data into SpaCy's binary format (DocBin), which is required for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffaade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spacy_doc(file, data):\n",
    "    \"\"\"\n",
    "    Convert annotated data to SpaCy DocBin format.\n",
    "    \n",
    "    Args:\n",
    "        file: File handle for logging errors\n",
    "        data: List of tuples (text, {\"entities\": [(start, end, label), ...]})\n",
    "    \n",
    "    Returns:\n",
    "        DocBin: SpaCy DocBin object containing processed documents\n",
    "    \"\"\"\n",
    "    # Create a blank Spanish language model\n",
    "    nlp = spacy.blank('es')\n",
    "    db = DocBin()\n",
    "    \n",
    "    # Process each annotated text\n",
    "    for text, annot in tqdm(data):\n",
    "        doc = nlp.make_doc(text)\n",
    "        entities = annot['entities']\n",
    "        \n",
    "        ents = []\n",
    "        entity_indices = []\n",
    "        \n",
    "        # Convert character offsets to SpaCy spans\n",
    "        for start, end, label in entities:\n",
    "            # Skip overlapping entities\n",
    "            skip_entity = False\n",
    "            for idx in range(start, end):\n",
    "                if idx in entity_indices:\n",
    "                    skip_entity = True\n",
    "                    break\n",
    "            if skip_entity:\n",
    "                continue\n",
    "            \n",
    "            # Track character indices to detect overlaps\n",
    "            entity_indices = entity_indices + list(range(start, end))\n",
    "            \n",
    "            try:\n",
    "                # Create span with strict alignment\n",
    "                span = doc.char_span(start, end, label=label, alignment_mode='strict')\n",
    "            except Exception as e:\n",
    "                # Log any errors during span creation\n",
    "                continue\n",
    "            \n",
    "            if span is None:\n",
    "                # Log annotations that couldn't be aligned\n",
    "                err_data = f\"{start},{end}: {text}\\n\"\n",
    "                file.write(err_data)\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        \n",
    "        # Add entities to document\n",
    "        try:\n",
    "            doc.ents = ents\n",
    "            db.add(doc)\n",
    "        except Exception as e:\n",
    "            # Skip documents that cause errors\n",
    "            pass\n",
    "    \n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81lsb9wyki9",
   "metadata": {},
   "source": [
    "## Step 6: Prepare Training and Test Data\n",
    "\n",
    "Split the data into training and test sets, then convert them to SpaCy format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee184036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training (80%) and testing (20%) sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(cv_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training examples: {len(train)}\")\n",
    "print(f\"Testing examples: {len(test)}\")\n",
    "\n",
    "# Define output directory for training files\n",
    "output_dir = '/content/drive/MyDrive/Custom_NER/trained_models/'\n",
    "\n",
    "# Open error log file\n",
    "file = open(f'{output_dir}train_file.txt', 'w')\n",
    "\n",
    "# Convert training data to SpaCy format\n",
    "print(\"\\nProcessing training data...\")\n",
    "db = get_spacy_doc(file, train)\n",
    "db.to_disk(f'{output_dir}train_data.spacy')\n",
    "\n",
    "# Convert test data to SpaCy format\n",
    "print(\"Processing test data...\")\n",
    "db = get_spacy_doc(file, test)\n",
    "db.to_disk(f'{output_dir}test_data.spacy')\n",
    "\n",
    "# Close error log\n",
    "file.close()\n",
    "\n",
    "print(\"\\nData preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j3hz7o2l12f",
   "metadata": {},
   "source": [
    "## Step 7: Train the NER Model\n",
    "\n",
    "Now we'll train the model using the SpaCy CLI. This process may take some time depending on your dataset size and GPU availability.\n",
    "\n",
    "**Note:** Make sure all paths are consistent with your directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599cc033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the NER model\n",
    "# Update paths to match your directory structure\n",
    "!python -m spacy train \\\n",
    "  /content/drive/MyDrive/Custom_NER/config/config.cfg \\\n",
    "  --output /content/drive/MyDrive/Custom_NER/trained_models/output \\\n",
    "  --paths.train /content/drive/MyDrive/Custom_NER/trained_models/train_data.spacy \\\n",
    "  --paths.dev /content/drive/MyDrive/Custom_NER/trained_models/test_data.spacy \\\n",
    "  --gpu-id 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2too1qkfq3x",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Inference and Testing\n",
    "\n",
    "After training is complete, you can use the trained model to extract entities from new text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de02f46",
   "metadata": {},
   "source": [
    "## Step 8: Load the Trained Model\n",
    "\n",
    "Load the best performing model from the training output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e10eedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained NER model\n",
    "# Use the same path as specified in the training step\n",
    "nlp = spacy.load('/content/drive/MyDrive/Custom_NER/trained_models/output/model-best')\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Pipeline components: {nlp.pipe_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5qwyue6djru",
   "metadata": {},
   "source": [
    "## Step 9: Run Inference on Sample Text\n",
    "\n",
    "Test the model by extracting named entities from a sample Spanish text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ab2014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Spanish text for testing\n",
    "text = \"\"\"Los ministros de Cultura, Leslie Urteaga, de Comercio Exterior y Turismo, Juan Carlos Mathews y de Ambiente, Albina Ruíz, no llegaron a obtener acuerdos con las organizaciones sociales que se mantienen en huelga por la venta de entradas virtuales a Machu Picchu. La ministra, Leslie Urteaga, indicó que no se ha solicitado una tregua, lo que se comunicó era que la mesa se instalaba siempre y cuando la huelga se paralizaba. La titular del sector, planteó que sea la PCM la instancia que se encargue de la venta de entradas virtuales. El primer ministro, Alberto Otárola, manifestó que debe haber orden y recordó que habrían sanciones penales para los que bloqueen carreteras. Otros países han recomendado no viajar a Cusco. La periodista María Teresa Braschi, indicó que la Embajada de Estados Unidos, también recomendaron no viajar a Cusco.\"\"\"\n",
    "\n",
    "# Process the text with the trained model\n",
    "doc = nlp(text)\n",
    "\n",
    "# Display extracted entities\n",
    "print(\"Extracted Named Entities:\\n\")\n",
    "print(f\"{'Entity':<40} {'Label':<15}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:<40} {ent.label_:<15}\")\n",
    "\n",
    "# Display entity counts\n",
    "print(f\"\\n\\nTotal entities found: {len(doc.ents)}\")\n",
    "entity_counts = {}\n",
    "for ent in doc.ents:\n",
    "    entity_counts[ent.label_] = entity_counts.get(ent.label_, 0) + 1\n",
    "\n",
    "print(\"\\nEntity distribution:\")\n",
    "for label, count in entity_counts.items():\n",
    "    print(f\"  {label}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
